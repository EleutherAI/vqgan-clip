{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Mse regulized VQGANCLIP_zquantize public.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8e3c597a5a284ccdaef3fce2c61c5109": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5f30f35530cb47cebffc834a6c8a35d3",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8c4e8f674fdf45ecadab6692a791c7eb",
              "IPY_MODEL_0a293b68b3fc42a7a32e5e43464f187a"
            ]
          }
        },
        "5f30f35530cb47cebffc834a6c8a35d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8c4e8f674fdf45ecadab6692a791c7eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ce9eb22296cf446baf5aa37b7f5c8762",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4841acc9dc494a64b38e4caccd88312d"
          }
        },
        "0a293b68b3fc42a7a32e5e43464f187a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_bddb899ed004411db335f93956ab5dfb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 458/? [05:04&lt;00:00,  1.51it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_561eec4d1a6240a8ba347a04247cb3cd"
          }
        },
        "ce9eb22296cf446baf5aa37b7f5c8762": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4841acc9dc494a64b38e4caccd88312d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bddb899ed004411db335f93956ab5dfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "561eec4d1a6240a8ba347a04247cb3cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Generates images from text prompts with VQGAN and CLIP (z+quantize method).\r\n",
        "\r\n",
        "By jbustter https://twitter.com/jbusted1 .\r\n",
        "Based on a notebook by Katherine Crowson (https://github.com/crowsonkb, https://twitter.com/RiversHaveWings)\r\n"
      ],
      "metadata": {
        "id": "CppIQlPhhwhs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!nvidia-smi"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkUfzT60ZZ9q",
        "outputId": "755810a3-8b3a-47f4-b99a-eddeb5d98eaa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!git clone https://github.com/openai/CLIP\r\n",
        "!git clone https://github.com/CompVis/taming-transformers\r\n",
        "!pip install ftfy regex tqdm omegaconf pytorch-lightning\r\n",
        "!pip install kornia\r\n",
        "!pip install einops"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wSfISAhyPmyp",
        "outputId": "257d96b4-544a-4975-c402-345ef3557b8c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "\r\n",
        "!curl -L 'https://heibox.uni-heidelberg.de/d/a7530b09fed84f80a887/files/?p=%2Fconfigs%2Fmodel.yaml&dl=1' > vqgan_imagenet_f16_16384.yaml\r\n",
        "!curl -L 'https://heibox.uni-heidelberg.de/d/a7530b09fed84f80a887/files/?p=%2Fckpts%2Flast.ckpt&dl=1' > vqgan_imagenet_f16_16384.ckpt\r\n"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhhdWrSxQhwg",
        "outputId": "c29c50cc-efe4-40c8-ffee-fc76ff6aa518"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "import argparse\r\n",
        "import math\r\n",
        "from pathlib import Path\r\n",
        "import sys\r\n",
        "\r\n",
        "sys.path.append('./taming-transformers')\r\n",
        "\r\n",
        "from IPython import display\r\n",
        "from omegaconf import OmegaConf\r\n",
        "from PIL import Image\r\n",
        "from taming.models import cond_transformer, vqgan\r\n",
        "import torch\r\n",
        "from torch import nn, optim\r\n",
        "from torch.nn import functional as F\r\n",
        "from torchvision import transforms\r\n",
        "from torchvision.transforms import functional as TF\r\n",
        "from tqdm.notebook import tqdm\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "from CLIP import clip\r\n",
        "\r\n",
        "import kornia.augmentation as K"
      ],
      "outputs": [],
      "metadata": {
        "id": "EXMSuW2EQWsd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "def noise_gen(shape):\r\n",
        "    n, c, h, w = shape\r\n",
        "    noise = torch.zeros([n, c, 1, 1])\r\n",
        "    for i in reversed(range(5)):\r\n",
        "        h_cur, w_cur = h // 2**i, w // 2**i\r\n",
        "        noise = F.interpolate(noise, (h_cur, w_cur), mode='bicubic', align_corners=False)\r\n",
        "        noise += torch.randn([n, c, h_cur, w_cur]) / 5\r\n",
        "    return noise\r\n",
        "\r\n",
        "\r\n",
        "def sinc(x):\r\n",
        "    return torch.where(x != 0, torch.sin(math.pi * x) / (math.pi * x), x.new_ones([]))\r\n",
        "\r\n",
        "\r\n",
        "def lanczos(x, a):\r\n",
        "    cond = torch.logical_and(-a < x, x < a)\r\n",
        "    out = torch.where(cond, sinc(x) * sinc(x/a), x.new_zeros([]))\r\n",
        "    return out / out.sum()\r\n",
        "\r\n",
        "\r\n",
        "def ramp(ratio, width):\r\n",
        "    n = math.ceil(width / ratio + 1)\r\n",
        "    out = torch.empty([n])\r\n",
        "    cur = 0\r\n",
        "    for i in range(out.shape[0]):\r\n",
        "        out[i] = cur\r\n",
        "        cur += ratio\r\n",
        "    return torch.cat([-out[1:].flip([0]), out])[1:-1]\r\n",
        "\r\n",
        "\r\n",
        "def resample(input, size, align_corners=True):\r\n",
        "    n, c, h, w = input.shape\r\n",
        "    dh, dw = size\r\n",
        "\r\n",
        "    input = input.view([n * c, 1, h, w])\r\n",
        "\r\n",
        "    if dh < h:\r\n",
        "        kernel_h = lanczos(ramp(dh / h, 2), 2).to(input.device, input.dtype)\r\n",
        "        pad_h = (kernel_h.shape[0] - 1) // 2\r\n",
        "        input = F.pad(input, (0, 0, pad_h, pad_h), 'reflect')\r\n",
        "        input = F.conv2d(input, kernel_h[None, None, :, None])\r\n",
        "\r\n",
        "    if dw < w:\r\n",
        "        kernel_w = lanczos(ramp(dw / w, 2), 2).to(input.device, input.dtype)\r\n",
        "        pad_w = (kernel_w.shape[0] - 1) // 2\r\n",
        "        input = F.pad(input, (pad_w, pad_w, 0, 0), 'reflect')\r\n",
        "        input = F.conv2d(input, kernel_w[None, None, None, :])\r\n",
        "\r\n",
        "    input = input.view([n, c, h, w])\r\n",
        "    return F.interpolate(input, size, mode='bicubic', align_corners=align_corners)\r\n",
        "    \r\n",
        "\r\n",
        "# def replace_grad(fake, real):\r\n",
        "#     return fake.detach() - real.detach() + real\r\n",
        "\r\n",
        "\r\n",
        "class ReplaceGrad(torch.autograd.Function):\r\n",
        "    @staticmethod\r\n",
        "    def forward(ctx, x_forward, x_backward):\r\n",
        "        ctx.shape = x_backward.shape\r\n",
        "        return x_forward\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def backward(ctx, grad_in):\r\n",
        "        return None, grad_in.sum_to_size(ctx.shape)\r\n",
        "\r\n",
        "\r\n",
        "class ClampWithGrad(torch.autograd.Function):\r\n",
        "    @staticmethod\r\n",
        "    def forward(ctx, input, min, max):\r\n",
        "        ctx.min = min\r\n",
        "        ctx.max = max\r\n",
        "        ctx.save_for_backward(input)\r\n",
        "        return input.clamp(min, max)\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def backward(ctx, grad_in):\r\n",
        "        input, = ctx.saved_tensors\r\n",
        "        return grad_in * (grad_in * (input - input.clamp(ctx.min, ctx.max)) >= 0), None, None\r\n",
        "\r\n",
        "replace_grad = ReplaceGrad.apply\r\n",
        "\r\n",
        "clamp_with_grad = ClampWithGrad.apply\r\n",
        "# clamp_with_grad = torch.clamp\r\n",
        "\r\n",
        "def vector_quantize(x, codebook):\r\n",
        "    d = x.pow(2).sum(dim=-1, keepdim=True) + codebook.pow(2).sum(dim=1) - 2 * x @ codebook.T\r\n",
        "    indices = d.argmin(-1)\r\n",
        "    x_q = F.one_hot(indices, codebook.shape[0]).to(d.dtype) @ codebook\r\n",
        "    return replace_grad(x_q, x)\r\n",
        "\r\n",
        "\r\n",
        "class Prompt(nn.Module):\r\n",
        "    def __init__(self, embed, weight=1., stop=float('-inf')):\r\n",
        "        super().__init__()\r\n",
        "        self.register_buffer('embed', embed)\r\n",
        "        self.register_buffer('weight', torch.as_tensor(weight))\r\n",
        "        self.register_buffer('stop', torch.as_tensor(stop))\r\n",
        "\r\n",
        "    def forward(self, input):\r\n",
        "        \r\n",
        "        input_normed = F.normalize(input.unsqueeze(1), dim=2)\r\n",
        "        embed_normed = F.normalize((self.embed).unsqueeze(0), dim=2)\r\n",
        "\r\n",
        "        dists = input_normed.sub(embed_normed).norm(dim=2).div(2).arcsin().pow(2).mul(2)\r\n",
        "        dists = dists * self.weight.sign()\r\n",
        "        return self.weight.abs() * replace_grad(dists, torch.maximum(dists, self.stop)).mean()\r\n",
        "\r\n",
        "\r\n",
        "def parse_prompt(prompt):\r\n",
        "    vals = prompt.rsplit(':', 2)\r\n",
        "    vals = vals + ['', '1', '-inf'][len(vals):]\r\n",
        "    return vals[0], float(vals[1]), float(vals[2])\r\n",
        "\r\n",
        "def one_sided_clip_loss(input, target, labels=None, logit_scale=100):\r\n",
        "    input_normed = F.normalize(input, dim=-1)\r\n",
        "    target_normed = F.normalize(target, dim=-1)\r\n",
        "    logits = input_normed @ target_normed.T * logit_scale\r\n",
        "    if labels is None:\r\n",
        "        labels = torch.arange(len(input), device=logits.device)\r\n",
        "    return F.cross_entropy(logits, labels)\r\n",
        "\r\n",
        "class MakeCutouts(nn.Module):\r\n",
        "    def __init__(self, cut_size, cutn, cut_pow=1.):\r\n",
        "        super().__init__()\r\n",
        "        self.cut_size = cut_size\r\n",
        "        self.cutn = cutn\r\n",
        "        self.cut_pow = cut_pow\r\n",
        "\r\n",
        "        self.av_pool = nn.AdaptiveAvgPool2d((self.cut_size, self.cut_size))\r\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d((self.cut_size, self.cut_size))\r\n",
        "\r\n",
        "    def set_cut_pow(self, cut_pow):\r\n",
        "      self.cut_pow = cut_pow\r\n",
        "\r\n",
        "    def forward(self, input):\r\n",
        "        sideY, sideX = input.shape[2:4]\r\n",
        "        max_size = min(sideX, sideY)\r\n",
        "        min_size = min(sideX, sideY, self.cut_size)\r\n",
        "        cutouts = []\r\n",
        "        cutouts_full = []\r\n",
        "        \r\n",
        "        min_size_width = min(sideX, sideY)\r\n",
        "        lower_bound = float(self.cut_size/min_size_width)\r\n",
        "        \r\n",
        "        for ii in range(self.cutn):\r\n",
        "            \r\n",
        "            \r\n",
        "          size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\r\n",
        "\r\n",
        "          offsetx = torch.randint(0, sideX - size + 1, ())\r\n",
        "          offsety = torch.randint(0, sideY - size + 1, ())\r\n",
        "          cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\r\n",
        "          cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\r\n",
        "\r\n",
        "        \r\n",
        "        cutouts = torch.cat(cutouts, dim=0)\r\n",
        "\r\n",
        "        if args.use_augs:\r\n",
        "          cutouts = augs(cutouts)\r\n",
        "\r\n",
        "        if args.noise_fac:\r\n",
        "          facs = cutouts.new_empty([cutouts.shape[0], 1, 1, 1]).uniform_(0, args.noise_fac)\r\n",
        "          cutouts = cutouts + facs * torch.randn_like(cutouts)\r\n",
        "        \r\n",
        "\r\n",
        "        return clamp_with_grad(cutouts, 0, 1)\r\n",
        "\r\n",
        "\r\n",
        "def load_vqgan_model(config_path, checkpoint_path):\r\n",
        "    config = OmegaConf.load(config_path)\r\n",
        "    if config.model.target == 'taming.models.vqgan.VQModel':\r\n",
        "        model = vqgan.VQModel(**config.model.params)\r\n",
        "        model.eval().requires_grad_(False)\r\n",
        "        model.init_from_ckpt(checkpoint_path)\r\n",
        "    elif config.model.target == 'taming.models.cond_transformer.Net2NetTransformer':\r\n",
        "        parent_model = cond_transformer.Net2NetTransformer(**config.model.params)\r\n",
        "        parent_model.eval().requires_grad_(False)\r\n",
        "        parent_model.init_from_ckpt(checkpoint_path)\r\n",
        "        model = parent_model.first_stage_model\r\n",
        "    elif config.model.target == 'taming.models.vqgan.GumbelVQ':\r\n",
        "        model = vqgan.GumbelVQ(**config.model.params)\r\n",
        "        model.eval().requires_grad_(False)\r\n",
        "        model.init_from_ckpt(checkpoint_path)\r\n",
        "    else:\r\n",
        "        raise ValueError(f'unknown model type: {config.model.target}')\r\n",
        "    del model.loss\r\n",
        "    return model\r\n",
        "\r\n",
        "def resize_image(image, out_size):\r\n",
        "    ratio = image.size[0] / image.size[1]\r\n",
        "    area = min(image.size[0] * image.size[1], out_size[0] * out_size[1])\r\n",
        "    size = round((area * ratio)**0.5), round((area / ratio)**0.5)\r\n",
        "    return image.resize(size, Image.LANCZOS)\r\n",
        "\r\n",
        "class TVLoss(nn.Module):\r\n",
        "    def forward(self, input):\r\n",
        "        input = F.pad(input, (0, 1, 0, 1), 'replicate')\r\n",
        "        x_diff = input[..., :-1, 1:] - input[..., :-1, :-1]\r\n",
        "        y_diff = input[..., 1:, :-1] - input[..., :-1, :-1]\r\n",
        "        diff = x_diff**2 + y_diff**2 + 1e-8\r\n",
        "        return diff.mean(dim=1).sqrt().mean()\r\n",
        "\r\n",
        "class GaussianBlur2d(nn.Module):\r\n",
        "    def __init__(self, sigma, window=0, mode='reflect', value=0):\r\n",
        "        super().__init__()\r\n",
        "        self.mode = mode\r\n",
        "        self.value = value\r\n",
        "        if not window:\r\n",
        "            window = max(math.ceil((sigma * 6 + 1) / 2) * 2 - 1, 3)\r\n",
        "        if sigma:\r\n",
        "            kernel = torch.exp(-(torch.arange(window) - window // 2)**2 / 2 / sigma**2)\r\n",
        "            kernel /= kernel.sum()\r\n",
        "        else:\r\n",
        "            kernel = torch.ones([1])\r\n",
        "        self.register_buffer('kernel', kernel)\r\n",
        "\r\n",
        "    def forward(self, input):\r\n",
        "        n, c, h, w = input.shape\r\n",
        "        input = input.view([n * c, 1, h, w])\r\n",
        "        start_pad = (self.kernel.shape[0] - 1) // 2\r\n",
        "        end_pad = self.kernel.shape[0] // 2\r\n",
        "        input = F.pad(input, (start_pad, end_pad, start_pad, end_pad), self.mode, self.value)\r\n",
        "        input = F.conv2d(input, self.kernel[None, None, None, :])\r\n",
        "        input = F.conv2d(input, self.kernel[None, None, :, None])\r\n",
        "        return input.view([n, c, h, w])\r\n",
        "\r\n",
        "class EMATensor(nn.Module):\r\n",
        "    \"\"\"implmeneted by Katherine Crowson\"\"\"\r\n",
        "    def __init__(self, tensor, decay):\r\n",
        "        super().__init__()\r\n",
        "        self.tensor = nn.Parameter(tensor)\r\n",
        "        self.register_buffer('biased', torch.zeros_like(tensor))\r\n",
        "        self.register_buffer('average', torch.zeros_like(tensor))\r\n",
        "        self.decay = decay\r\n",
        "        self.register_buffer('accum', torch.tensor(1.))\r\n",
        "        self.update()\r\n",
        "    \r\n",
        "    @torch.no_grad()\r\n",
        "    def update(self):\r\n",
        "        if not self.training:\r\n",
        "            raise RuntimeError('update() should only be called during training')\r\n",
        "\r\n",
        "        self.accum *= self.decay\r\n",
        "        self.biased.mul_(self.decay)\r\n",
        "        self.biased.add_((1 - self.decay) * self.tensor)\r\n",
        "        self.average.copy_(self.biased)\r\n",
        "        self.average.div_(1 - self.accum)\r\n",
        "\r\n",
        "    def forward(self):\r\n",
        "        if self.training:\r\n",
        "            return self.tensor\r\n",
        "        return self.average\r\n",
        "\r\n",
        "%mkdir /content/vids"
      ],
      "outputs": [],
      "metadata": {
        "id": "JvnTBhPGT1gn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ARGS"
      ],
      "metadata": {
        "id": "WN4OtaLbHBN6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "source": [
        "args = argparse.Namespace(\r\n",
        "    \r\n",
        "    prompts=[\"a photo-realistic and beautiful painting of an old man sitting on a chair next to a giant iceberg and looking at a green lush landscape.\"],\r\n",
        "    size=[640, 512], \r\n",
        "    init_image= None,\r\n",
        "    init_weight= 1.5,\r\n",
        "\r\n",
        "    # clip model settings\r\n",
        "    clip_model='ViT-B/32',\r\n",
        "    vqgan_config='vqgan_imagenet_f16_16384.yaml',         \r\n",
        "    vqgan_checkpoint='vqgan_imagenet_f16_16384.ckpt',\r\n",
        "    step_size=0.95,\r\n",
        "    \r\n",
        "    # cutouts / crops\r\n",
        "    cutn=64,\r\n",
        "    cut_pow=1,\r\n",
        "\r\n",
        "    # display\r\n",
        "    display_freq=25,\r\n",
        "    seed=158758,\r\n",
        "    use_augs = True,\r\n",
        "    noise_fac= 0.1,\r\n",
        "\r\n",
        "    record_generation=True,\r\n",
        "\r\n",
        "    # noise and other constraints\r\n",
        "    use_noise = None,\r\n",
        "    constraint_regions = False,#\r\n",
        "    \r\n",
        "    \r\n",
        "    # add noise to embedding\r\n",
        "    noise_prompt_weights = None,\r\n",
        "    noise_prompt_seeds = [14575],#\r\n",
        "\r\n",
        "    # mse settings\r\n",
        "    mse_withzeros = True,\r\n",
        "    mse_decay_rate = 50,\r\n",
        "    mse_epoches = 5,\r\n",
        "\r\n",
        "    # end itteration\r\n",
        "    max_itter = -1,\r\n",
        ")\r\n",
        "\r\n",
        "mse_decay = 0\r\n",
        "if args.init_weight:\r\n",
        "  mse_decay = args.init_weight / args.mse_epoches\r\n",
        "\r\n",
        "# <AUGMENTATIONS>\r\n",
        "augs = nn.Sequential(\r\n",
        "    \r\n",
        "    K.RandomHorizontalFlip(p=0.5),\r\n",
        "    K.RandomAffine(degrees=30, translate=0.1, p=0.8, padding_mode='border'), # padding_mode=2\r\n",
        "    K.RandomPerspective(0.2,p=0.4, ),\r\n",
        "    K.ColorJitter(hue=0.01, saturation=0.01, p=0.7),\r\n",
        "\r\n",
        "    )\r\n",
        "\r\n",
        "noise = noise_gen([1, 3, args.size[0], args.size[1]])\r\n",
        "image = TF.to_pil_image(noise.div(5).add(0.5).clamp(0, 1)[0])\r\n",
        "image.save('init3.png')"
      ],
      "outputs": [],
      "metadata": {
        "id": "tLw9p5Rzacso"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Actually do the run..."
      ],
      "metadata": {
        "id": "QXgTa_JWi7Sn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\r\n",
        "\r\n",
        "print('Using device:', device)\r\n",
        "print('using prompts: ', args.prompts)\r\n",
        "\r\n",
        "tv_loss = TVLoss() \r\n",
        "\r\n",
        "model = load_vqgan_model(args.vqgan_config, args.vqgan_checkpoint).to(device)\r\n",
        "perceptor = clip.load(args.clip_model, jit=False)[0].eval().requires_grad_(False).to(device)\r\n",
        "mse_weight = args.init_weight\r\n",
        "\r\n",
        "cut_size = perceptor.visual.input_resolution\r\n",
        "# e_dim = model.quantize.e_dim\r\n",
        "\r\n",
        "if args.vqgan_checkpoint == 'vqgan_openimages_f16_8192.ckpt':\r\n",
        "    e_dim = 256\r\n",
        "    n_toks = model.quantize.n_embed\r\n",
        "    z_min = model.quantize.embed.weight.min(dim=0).values[None, :, None, None]\r\n",
        "    z_max = model.quantize.embed.weight.max(dim=0).values[None, :, None, None]\r\n",
        "else:\r\n",
        "    e_dim = model.quantize.e_dim\r\n",
        "    n_toks = model.quantize.n_e\r\n",
        "    z_min = model.quantize.embedding.weight.min(dim=0).values[None, :, None, None]\r\n",
        "    z_max = model.quantize.embedding.weight.max(dim=0).values[None, :, None, None]\r\n",
        "\r\n",
        "\r\n",
        "make_cutouts = MakeCutouts(cut_size, args.cutn, cut_pow=args.cut_pow)\r\n",
        "\r\n",
        "f = 2**(model.decoder.num_resolutions - 1)\r\n",
        "toksX, toksY = args.size[0] // f, args.size[1] // f\r\n",
        "\r\n",
        "if args.seed is not None:\r\n",
        "    torch.manual_seed(args.seed)\r\n",
        "\r\n",
        "if args.init_image:\r\n",
        "    pil_image = Image.open(args.init_image).convert('RGB')\r\n",
        "    pil_image = pil_image.resize((toksX * 16, toksY * 16), Image.LANCZOS)\r\n",
        "    pil_image = TF.to_tensor(pil_image)\r\n",
        "    if args.use_noise:\r\n",
        "      pil_image = pil_image + args.use_noise * torch.randn_like(pil_image) \r\n",
        "    z, *_ = model.encode(pil_image.to(device).unsqueeze(0) * 2 - 1)\r\n",
        "\r\n",
        "else:\r\n",
        "    \r\n",
        "    one_hot = F.one_hot(torch.randint(n_toks, [toksY * toksX], device=device), n_toks).float()\r\n",
        "\r\n",
        "    if args.vqgan_checkpoint == 'vqgan_openimages_f16_8192.ckpt':\r\n",
        "        z = one_hot @ model.quantize.embed.weight\r\n",
        "    else:\r\n",
        "        z = one_hot @ model.quantize.embedding.weight\r\n",
        "    z = z.view([-1, toksY, toksX, e_dim]).permute(0, 3, 1, 2)\r\n",
        "\r\n",
        "if args.mse_withzeros and not args.init_image:\r\n",
        "  z_orig = torch.zeros_like(z)\r\n",
        "else:\r\n",
        "  z_orig = z.clone()\r\n",
        "\r\n",
        "z.requires_grad = True\r\n",
        "\r\n",
        "opt = optim.Adam([z], lr=args.step_size, weight_decay=0.00000000)\r\n",
        "\r\n",
        "normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\r\n",
        "                                 std=[0.26862954, 0.26130258, 0.27577711])\r\n",
        "\r\n",
        "pMs = []\r\n",
        "\r\n",
        "if args.noise_prompt_weights and args.noise_prompt_seeds:\r\n",
        "  for seed, weight in zip(args.noise_prompt_seeds, args.noise_prompt_weights):\r\n",
        "    gen = torch.Generator().manual_seed(seed)\r\n",
        "    embed = torch.empty([1, perceptor.visual.output_dim]).normal_(generator=gen)\r\n",
        "    pMs.append(Prompt(embed, weight).to(device))\r\n",
        "\r\n",
        "for prompt in args.prompts:\r\n",
        "    txt, weight, stop = parse_prompt(prompt)\r\n",
        "    embed = perceptor.encode_text(clip.tokenize(txt).to(device)).float()\r\n",
        "    pMs.append(Prompt(embed, weight, stop).to(device))\r\n",
        "\r\n",
        "\r\n",
        "def synth(z, quantize=True):\r\n",
        "    if args.constraint_regions:\r\n",
        "      z = replace_grad(z, z * z_mask)\r\n",
        "\r\n",
        "    if quantize:\r\n",
        "      if args.vqgan_checkpoint == 'vqgan_openimages_f16_8192.ckpt':\r\n",
        "        z_q = vector_quantize(z.movedim(1, 3), model.quantize.embed.weight).movedim(3, 1)\r\n",
        "      else:\r\n",
        "        z_q = vector_quantize(z.movedim(1, 3), model.quantize.embedding.weight).movedim(3, 1)\r\n",
        "\r\n",
        "    else:\r\n",
        "      z_q = z.model\r\n",
        "\r\n",
        "    return clamp_with_grad(model.decode(z_q).add(1).div(2), 0, 1)\r\n",
        "\r\n",
        "@torch.no_grad()\r\n",
        "def checkin(i, losses):\r\n",
        "    losses_str = ', '.join(f'{loss.item():g}' for loss in losses)\r\n",
        "    tqdm.write(f'i: {i}, loss: {sum(losses).item():g}, losses: {losses_str}')\r\n",
        "    out = synth(z, True)\r\n",
        "\r\n",
        "    TF.to_pil_image(out[0].cpu()).save('progress.png')   \r\n",
        "    display.display(display.Image('progress.png')) \r\n",
        "\r\n",
        "\r\n",
        "def ascend_txt():\r\n",
        "    global mse_weight\r\n",
        "\r\n",
        "    out = synth(z)\r\n",
        "\r\n",
        "    if args.record_generation:\r\n",
        "      with torch.no_grad():\r\n",
        "        global vid_index\r\n",
        "        out_a = synth(z, True)\r\n",
        "        TF.to_pil_image(out_a[0].cpu()).save(f'/content/vids/{vid_index}.png')\r\n",
        "        vid_index += 1\r\n",
        "\r\n",
        "    cutouts = make_cutouts(out)\r\n",
        "\r\n",
        "    iii = perceptor.encode_image(normalize(cutouts)).float()\r\n",
        "\r\n",
        "    result = []\r\n",
        "\r\n",
        "    if args.init_weight:\r\n",
        "        \r\n",
        "        global z_orig\r\n",
        "        \r\n",
        "        result.append(F.mse_loss(z, z_orig) * mse_weight / 2)\r\n",
        "        # result.append(F.mse_loss(z, z_orig) * ((1/torch.tensor((i)*2 + 1))*mse_weight) / 2)\r\n",
        "\r\n",
        "        with torch.no_grad():\r\n",
        "          if i > 0 and i%args.mse_decay_rate==0 and i <= args.mse_decay_rate*args.mse_epoches:\r\n",
        "\r\n",
        "            if mse_weight - mse_decay > 0 and mse_weight - mse_decay >= mse_decay:\r\n",
        "              mse_weight = mse_weight - mse_decay\r\n",
        "              print(f\"updated mse weight: {mse_weight}\")\r\n",
        "            else:\r\n",
        "              mse_weight = 0\r\n",
        "              print(f\"updated mse weight: {mse_weight}\")\r\n",
        "\r\n",
        "    for prompt in pMs:\r\n",
        "        result.append(prompt(iii))\r\n",
        "\r\n",
        "    return result\r\n",
        "\r\n",
        "vid_index = 0\r\n",
        "def train(i):\r\n",
        "    \r\n",
        "    opt.zero_grad()\r\n",
        "    lossAll = ascend_txt()\r\n",
        "\r\n",
        "    if i % args.display_freq == 0:\r\n",
        "        checkin(i, lossAll)\r\n",
        "    \r\n",
        "    loss = sum(lossAll)\r\n",
        "\r\n",
        "    loss.backward()\r\n",
        "    opt.step()\r\n",
        "\r\n",
        "i = 0\r\n",
        "try:\r\n",
        "    with tqdm() as pbar:\r\n",
        "        while True and i != args.max_itter:\r\n",
        "\r\n",
        "            train(i)\r\n",
        "\r\n",
        "            if i > 0 and i%args.mse_decay_rate==0 and i <= args.mse_decay_rate * args.mse_epoches:\r\n",
        "              \r\n",
        "              opt = optim.Adam([z], lr=args.step_size, weight_decay=0.00000000)\r\n",
        "\r\n",
        "            i += 1\r\n",
        "            pbar.update()\r\n",
        "\r\n",
        "except KeyboardInterrupt:\r\n",
        "    pass\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "g7EDme5RYCrt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "8e3c597a5a284ccdaef3fce2c61c5109",
            "5f30f35530cb47cebffc834a6c8a35d3",
            "8c4e8f674fdf45ecadab6692a791c7eb",
            "0a293b68b3fc42a7a32e5e43464f187a",
            "ce9eb22296cf446baf5aa37b7f5c8762",
            "4841acc9dc494a64b38e4caccd88312d",
            "bddb899ed004411db335f93956ab5dfb",
            "561eec4d1a6240a8ba347a04247cb3cd"
          ]
        },
        "outputId": "c83a524b-9306-4b99-d1c8-b0302a3add0d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# create video"
      ],
      "metadata": {
        "id": "CDUaCaRnUKMZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "%cd vids\r\n",
        "\r\n",
        "images = \"%d.png\"\r\n",
        "video = \"/content/my_video.mp4\"\r\n",
        "!ffmpeg -r 30 -i $images -crf 20 -s 640x512 -pix_fmt yuv420p $video\r\n",
        " \r\n",
        "%cd ..\r\n",
        "\r\n",
        "from google.colab import files\r\n",
        "files.download('my_video.mp4')"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DT3hKb5gJUPq",
        "outputId": "43d63652-9810-4651-c733-3678839fdc1c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "delete all frames from folder"
      ],
      "metadata": {
        "id": "UiZMW3kAUD1f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "%cd vids\r\n",
        "%rm *.png\r\n",
        "%cd .."
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsixT6gqJ8aY",
        "outputId": "31aa3776-b614-4f48-e2d1-df7cfb69a31e"
      }
    }
  ]
}
