{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Semantic Style Transfer with CLIP+VQGAN (Gumbel VQGAN)",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-mqM09y4kKz"
      },
      "source": [
        "# Zero shot semantic style transfer\n",
        "Equal contribution:\n",
        "\n",
        "\n",
        "By \n",
        "\n",
        "Katherine Crowson (https://twitter.com/RiversHaveWings)\n",
        "\n",
        "Louis Castricato (https://twitter.com/lcastricato)\n",
        "\n",
        "Nev (https://twitter.com/apeoffire)\n",
        "\n",
        "Jbustter (https://twitter.com/jbusted1) \n",
        "\n",
        "Theodore (https://twitter.com/TheodoreGalanos)\n",
        "\n",
        "... and all of our friends at EleutherAI!\n",
        "\n",
        "Business end below is the area of interest. Masking is performed via a logit lens technique, optimization is performed via a spherical geodesic + reweighing technique. Currently limitations are mostly due to the restrictions imposed by our small CLIP model as well as various tweaks needed for masking. Masking is a bit finnicky. We will update this notebook in due time. \n",
        "\n",
        "When performing interactive editing, you'll need to keep reuploading the output back to imgur. We wanted the interactive editing to be non-stateful, so the z values are not preserved. This in turn allows for a more interactive experience than say StyleCLIP.\n",
        "\n",
        "# Version Number: 1.1\n",
        "\n",
        "Added: Support for custom GANs. Dynamically scaling masking.\n",
        "\n",
        "Coming soon: Improved masking. Better default GAN.\n",
        "*italicized text*\n",
        "Coming \"soon\": Whitepaper\n",
        "\n",
        "Coming less \"soon\": fatter CLIP text encoder. Currently WIP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDM3Ahvn5ntT",
        "cellView": "form"
      },
      "source": [
        "# @title Licensed under the MIT License\n",
        "\n",
        "# Copyright (c) 2021 Katherine Crowson\n",
        "\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "# of this software and associated documentation files (the \"Software\"), to deal\n",
        "# in the Software without restriction, including without limitation the rights\n",
        "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "# copies of the Software, and to permit persons to whom the Software is\n",
        "# furnished to do so, subject to the following conditions:\n",
        "\n",
        "# The above copyright notice and this permission notice shall be included in\n",
        "# all copies or substantial portions of the Software.\n",
        "\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
        "# THE SOFTWARE."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmIelvEgGzid"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suv9KMfDt5X3"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrmOXym9uI8m"
      },
      "source": [
        "!git clone https://github.com/openai/CLIP\n",
        "!git clone https://github.com/CompVis/taming-transformers\n",
        "!pip install ftfy regex tqdm omegaconf pytorch-lightning kornia madgrad einops"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2cOcZ_juVSv"
      },
      "source": [
        "!curl -L 'https://heibox.uni-heidelberg.de/d/2e5662443a6b4307b470/files/?p=%2Fconfigs%2Fmodel.yaml&dl=1' > vqgan_gumbel_f8_8192.yaml\n",
        "!curl -L 'https://heibox.uni-heidelberg.de/d/2e5662443a6b4307b470/files/?p=%2Fckpts%2Flast.ckpt&dl=1' > vqgan_gumbel_f8_8192.ckpt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmEzRVxMuYcZ"
      },
      "source": [
        "import math\n",
        "import io\n",
        "import sys\n",
        "\n",
        "from IPython import display\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "import requests\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import madgrad\n",
        "from torch.nn import functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import functional as TF\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "sys.path.append('./taming-transformers')\n",
        "\n",
        "from CLIP import clip\n",
        "from taming.models import cond_transformer, vqgan\n",
        "import kornia.augmentation as K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oO4ruO7pWPX8"
      },
      "source": [
        "# Masking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApT6Gsg2WSXl"
      },
      "source": [
        "\n",
        "class BoxCropper(object): \n",
        "    def __init__(self, w=0.3, h=0.3):\n",
        "      self.w, self.h = w, h\n",
        "\n",
        "    def sample(self, source):\n",
        "        w, h = int(source.width*self.w), int(source.height*self.h)\n",
        "        w, h = torch.randint(w//2, w+1, []).item(), torch.randint(h//2, h+1, []).item()\n",
        "        h = w\n",
        "        x1 = torch.randint(0, source.width - w + 1, []).item()\n",
        "        y1 = torch.randint(0, source.height - h + 1, []).item()\n",
        "        x2, y2 = x1 + w, y1 + h\n",
        "        box = x1, y1, x2, y2\n",
        "        crop = source.crop(box)\n",
        "        mask = torch.zeros([source.size[1], source.size[0]])\n",
        "        mask[y1:y2, x1:x2] = 1.\n",
        "        return crop, mask\n",
        "\n",
        "def sample(source, sampler, model, preprocess, n=64000, batch_size=128):\n",
        "    n_batches = 0- -n // batch_size  # round up\n",
        "    t_crop = 0\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for step in tqdm(range(n_batches)):\n",
        "            t_crop = float(step)/float(n_batches)\n",
        "            crop_cur = (0.4) * (1- t_crop) + (0.1) * t_crop\n",
        "            sampler.w = crop_cur\n",
        "            sampler.h = crop_cur\n",
        "\n",
        "            batch = []\n",
        "            for _ in range(batch_size):\n",
        "                crop, mask = sampler.sample(source)\n",
        "                batch.append((preprocess(crop).unsqueeze(0).to(next(model.parameters()).device), mask))\n",
        "            crops = torch.cat([img for img, *_ in batch], axis=0)\n",
        "            embeddings = model.encode_image(crops).cpu().detach()\n",
        "            # yield *zip(embeddings, [mask for _, mask, *_ in batch])\n",
        "            for emb, msk in zip(embeddings, [mask for _, mask, *_ in batch]):\n",
        "                yield emb, msk\n",
        "    # return samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g94tHqbmWUH4"
      },
      "source": [
        "def aggregate(samples, labels, model):\n",
        "    texts = clip.tokenize(labels).to(device)\n",
        "    with torch.no_grad():\n",
        "        text_embeddings = model.encode_text(texts).cpu()\n",
        "    masks = []\n",
        "    for label, text_emb in zip(labels, text_embeddings):\n",
        "        text_features = text_emb / text_emb.norm(dim=-1, keepdim=True)\n",
        "        pixel_sum = torch.ones_like(next(samples)[1])\n",
        "        samples_per_pixel = torch.ones_like(next(samples)[1])\n",
        "        # dists = [spherical_dist(text_emb.float(), embedding.float()).item()\n",
        "        #          for embedding, *_ in samples]\n",
        "        # min_dist, max_dist = min(dists), max(dists)\n",
        "        for embedding, mask in samples: # dist, (embedding, mask) in zip(dists, samples):\n",
        "            image_features = embedding / embedding.norm(dim=-1, keepdim=True)\n",
        "            logit_scale = model.logit_scale.exp().to(image_features.device)\n",
        "            logits_per_image = logit_scale * image_features @ text_features.t()\n",
        "            dist = logits_per_image.float().exp().item()\n",
        "            # dist = spherical_dist(text_emb.float(), embedding.float()).item()\n",
        "            pixel_sum += mask * dist\n",
        "            samples_per_pixel += mask\n",
        "        img = (#samples_per_pixel-\n",
        "               pixel_sum\n",
        "               ) / samples_per_pixel\n",
        "        # img *= 4\n",
        "        # print(img.max())\n",
        "        # print(img.min(), img.max())\n",
        "        img = ((img - img.min()\n",
        "        ) / img.max()) ** 2 # 0.75\n",
        "        # img /= img.max()\n",
        "        #img[img <= 0.001] = 0.\n",
        "        masks.append((img, label))\n",
        "    return masks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sd6xisXMWbNN"
      },
      "source": [
        "def visualise(source, masks):\n",
        "    source = TF.to_tensor(source)\n",
        "    for img, label in masks:\n",
        "        TF.to_pil_image(source * img[None]).save('mask_temp.png')\n",
        "        display.display(display.Image('mask_temp.png'))\n",
        "\n",
        "def save(masks):\n",
        "    source = torch.ones_like(masks[0])\n",
        "    for img, label in masks:\n",
        "        return source * img[None]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1u8d3mqWoRV"
      },
      "source": [
        "# Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tt9y78J1uysh"
      },
      "source": [
        "def sinc(x):\n",
        "    return torch.where(x != 0, torch.sin(math.pi * x) / (math.pi * x), x.new_ones([]))\n",
        "\n",
        "\n",
        "def lanczos(x, a):\n",
        "    cond = torch.logical_and(-a < x, x < a)\n",
        "    out = torch.where(cond, sinc(x) * sinc(x/a), x.new_zeros([]))\n",
        "    return out / out.sum()\n",
        "\n",
        "\n",
        "def ramp(ratio, width):\n",
        "    n = math.ceil(width / ratio + 1)\n",
        "    out = torch.empty([n])\n",
        "    cur = 0\n",
        "    for i in range(out.shape[0]):\n",
        "        out[i] = cur\n",
        "        cur += ratio\n",
        "    return torch.cat([-out[1:].flip([0]), out])[1:-1]\n",
        "\n",
        "\n",
        "def resample(input, size, align_corners=True):\n",
        "    n, c, h, w = input.shape\n",
        "    dh, dw = size\n",
        "\n",
        "    input = input.view([n * c, 1, h, w])\n",
        "\n",
        "    if dh < h:\n",
        "        kernel_h = lanczos(ramp(dh / h, 2), 2).to(input.device, input.dtype)\n",
        "        pad_h = (kernel_h.shape[0] - 1) // 2\n",
        "        input = F.pad(input, (0, 0, pad_h, pad_h), 'reflect')\n",
        "        input = F.conv2d(input, kernel_h[None, None, :, None])\n",
        "\n",
        "    if dw < w:\n",
        "        kernel_w = lanczos(ramp(dw / w, 2), 2).to(input.device, input.dtype)\n",
        "        pad_w = (kernel_w.shape[0] - 1) // 2\n",
        "        input = F.pad(input, (pad_w, pad_w, 0, 0), 'reflect')\n",
        "        input = F.conv2d(input, kernel_w[None, None, None, :])\n",
        "\n",
        "    input = input.view([n, c, h, w])\n",
        "    return F.interpolate(input, size, mode='bicubic', align_corners=align_corners)\n",
        "\n",
        "\n",
        "class ReplaceGrad(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x_forward, x_backward):\n",
        "        ctx.shape = x_backward.shape\n",
        "        return x_forward\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_in):\n",
        "        return None, grad_in.sum_to_size(ctx.shape)\n",
        "\n",
        "\n",
        "replace_grad = ReplaceGrad.apply\n",
        "\n",
        "\n",
        "class ClampWithGrad(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, min, max):\n",
        "        ctx.min = min\n",
        "        ctx.max = max\n",
        "        ctx.save_for_backward(input)\n",
        "        return input.clamp(min, max)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_in):\n",
        "        input, = ctx.saved_tensors\n",
        "        return grad_in * (grad_in * (input - input.clamp(ctx.min, ctx.max)) >= 0), None, None\n",
        "\n",
        "\n",
        "clamp_with_grad = ClampWithGrad.apply\n",
        "\n",
        "\n",
        "def spherical_dist(x, y, noise = False, noise_coeff=0.1):\n",
        "    x_normed = F.normalize(x, dim=-1)\n",
        "    y_normed = F.normalize(y, dim=-1)\n",
        "    if noise:\n",
        "        with torch.no_grad():\n",
        "            noise1 = torch.empty(x_normed.shape).normal_(0,0.0422).to(x_normed).detach()*noise_coeff\n",
        "            noise2 = torch.empty(y_normed.shape).normal_(0,0.0422).to(x_normed).detach()*noise_coeff\n",
        "\n",
        "            x_normed += noise1\n",
        "            y_normed += noise2\n",
        "    x_normed = F.normalize(x_normed, dim=-1)\n",
        "    y_normed = F.normalize(y_normed, dim=-1)\n",
        "\n",
        "    return x_normed.sub(y_normed).norm(dim=-1).div(2).arcsin().pow(2).mul(2)\n",
        "    \n",
        "def bdot(a, b):\n",
        "    B = a.shape[0]\n",
        "    S = a.shape[1]\n",
        "    b = b.expand(B, -1)\n",
        "    #print(a.shape)\n",
        "    #print(b.shape)\n",
        "    return torch.bmm(a.view(B, 1, S), b.view(B, S, 1)).reshape(-1)\n",
        "\n",
        "def inner_dist(x,y):\n",
        "    x_normed = F.normalize(x, dim=-1)\n",
        "    y_normed = F.normalize(y, dim=-1)\n",
        "    return bdot(x_normed, y_normed)\n",
        "\n",
        "class MakeCutouts(nn.Module):\n",
        "    def __init__(self, cut_size, cutn, cut_pow=1.):\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        self.cutn = cutn\n",
        "        self.cut_pow = cut_pow\n",
        "        self.noise_fac = 0.1\n",
        "        self.augs = nn.Sequential(\n",
        "          K.RandomHorizontalFlip(p=0.5),\n",
        "          K.RandomAffine(degrees=30, translate=0.1, p=0.8, padding_mode='border'),\n",
        "          K.RandomPerspective(0.2, p=0.4),\n",
        "          K.ColorJitter(hue=0.01, saturation=0.01, p=0.7),\n",
        "          K.RandomGrayscale(p=0.1),\n",
        "        )\n",
        "\n",
        "    def set_cut_pow(self, cut_pow):\n",
        "      self.cut_pow = cut_pow\n",
        "\n",
        "    def forward(self, input, cut_pow=None, augs=True, grads=True):\n",
        "        if cut_pow is None:\n",
        "          cut_pow = self.cut_pow\n",
        "\n",
        "        sideY, sideX = input.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "        min_size = min(sideX, sideY, self.cut_size)\n",
        "        cutouts = []\n",
        "        for _ in range(self.cutn):\n",
        "            size = int(torch.rand([])** cut_pow * (max_size - min_size) + min_size)\n",
        "            offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "            offsety = torch.randint(0, sideY - size + 1, ())\n",
        "            cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "            cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n",
        "        batch = torch.cat(cutouts, dim=0)\n",
        "        if grads:\n",
        "          batch = clamp_with_grad(batch, 0, 1)\n",
        "        if augs:\n",
        "          batch = self.augs(batch)\n",
        "          if self.noise_fac:\n",
        "              facs = batch.new_empty([self.cutn, 1, 1, 1]).uniform_(0, self.noise_fac)\n",
        "              batch = batch + facs * torch.randn_like(batch)\n",
        "        return batch\n",
        "\n",
        "\n",
        "\n",
        "def load_vqgan_model(config_path, checkpoint_path):\n",
        "    config = OmegaConf.load(config_path)\n",
        "    ddconfig = config.model.params.ddconfig\n",
        "    model = vqgan.GumbelVQ(**config.model.params)\n",
        "    model.init_from_ckpt(checkpoint_path)\n",
        "    model.eval().requires_grad_(False)\n",
        "    del model.loss\n",
        "    return model\n",
        "\n",
        "\n",
        "def size_to_fit(size, max_dim, scale_up=False):\n",
        "    w, h = size\n",
        "    if not scale_up and max(h, w) <= max_dim:\n",
        "        return w, h\n",
        "    new_w, new_h = max_dim, max_dim\n",
        "    if h > w:\n",
        "        new_w = round(max_dim * w / h)\n",
        "    else:\n",
        "        new_h = round(max_dim * h / w)\n",
        "    return new_w, new_h\n",
        "\n",
        "\n",
        "def fetch(url_or_path):\n",
        "    if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n",
        "        r = requests.get(url_or_path)\n",
        "        r.raise_for_status()\n",
        "        fd = io.BytesIO()\n",
        "        fd.write(r.content)\n",
        "        fd.seek(0)\n",
        "        return fd\n",
        "    return open(url_or_path, 'rb')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WwFQKt0ZDqB"
      },
      "source": [
        "## Business End\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbQFgQmvrRKu"
      },
      "source": [
        "Image path is the image you want to apply style transfer to (upload it to imgur). From text is the subject within the image you want to transfer (the thing you want the model to semantically segment to) and to text is the effect you want to apply.\n",
        "\n",
        "For example: \n",
        "From text as house and to text as carnival would segment out a house within the image and redraw it as a carnival."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Kj3hHrGvO3-"
      },
      "source": [
        "#@title Parameters\n",
        "\n",
        "from_image_path = 'https://i.imgur.com/7k0YQWK.png'  #@param {type:\"string\"}\n",
        "image_size = 640  #@param {type:\"integer\"}\n",
        "from_text = 'the snowy mountain'  #@param {type:\"string\"}\n",
        "to_text = 'The carnival is back in town!'  #@param {type:\"string\"}\n",
        "scale_dir_by =   1.25#@param {type:\"number\"}\n",
        "clip_model = 'ViT-B/32'  #@param [\"ViT-B/32\", \"ViT-B/16\", \"RN50\", \"RN101\", \"RN50x4\", \"RN50x16\"]\n",
        "use_mask = True  #@param {type:\"boolean\"}\n",
        "invert_mask = True  #@param {type:\"boolean\"}\n",
        "cut_pow_start = 0.3 #@param {type:\"number\"}\n",
        "cut_pow_end =  1.0#@param {type:\"number\"}\n",
        "cut_pow_length =  400#@param {type:\"integer\"}\n",
        "mask_samples =  16000#@param {type:\"integer\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXXiMRfsWxu6"
      },
      "source": [
        "#Reset tqdm\n",
        "#tqdm._instances.clear()\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "#Set up model and devices\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "torch.cuda.empty_cache()\n",
        "sdbo= scale_dir_by\n",
        "t = 0\n",
        "cut_out_num=64\n",
        "\n",
        "model = load_vqgan_model('vqgan_gumbel_f8_8192.yaml', 'vqgan_gumbel_f8_8192.ckpt').to(device)\n",
        "\n",
        "perceptor, preprocess = clip.load(clip_model, jit=False)\n",
        "perceptor.eval().requires_grad_(False).to(device)\n",
        "\n",
        "\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "                                 std=[0.26862954, 0.26130258, 0.27577711])\n",
        "\n",
        "cut_size = perceptor.visual.input_resolution\n",
        "f = 2**(model.decoder.num_resolutions - 1)\n",
        "make_cutouts = MakeCutouts(cut_size, cut_out_num, cut_pow=cut_pow_start)\n",
        "\n",
        "pil_image = Image.open(fetch(from_image_path)).convert('RGB')\n",
        "\n",
        "source = pil_image\n",
        "labels = [from_text]\n",
        "texts = clip.tokenize(labels).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7asCmJMnXanl"
      },
      "source": [
        "#Visualize the semantic segment we're going to use\n",
        "agg = aggregate(sample(source, BoxCropper(), perceptor, preprocess, n = mask_samples), labels, perceptor)\n",
        "visualise(source, agg)\n",
        "\n",
        "#Save this as mask.png\n",
        "new_p = TF.to_pil_image(agg[0][0])\n",
        "new_p.save('mask.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcpBoDSzwMLL"
      },
      "source": [
        "#Properly rescale image\n",
        "sideX, sideY = size_to_fit(pil_image.size, image_size, True)\n",
        "toksX, toksY = sideX // f, sideY // f\n",
        "sideX, sideY = toksX * f, toksY * f\n",
        "\n",
        "from_embed = perceptor.encode_text(clip.tokenize(from_text).to(device)).float()\n",
        "to_embed = perceptor.encode_text(clip.tokenize(to_text).to(device)).float()\n",
        "\n",
        "image = TF.to_tensor(pil_image.resize((sideX, sideY), Image.LANCZOS)).to(device).unsqueeze(0)\n",
        "mask_dist = None\n",
        "mask_total = 0.\n",
        "\n",
        "pil_mask = Image.open(\"mask.png\")\n",
        "#Are we using the mask we just generated?\n",
        "if use_mask:\n",
        "    if 'A' in pil_mask.getbands():\n",
        "        pil_mask = pil_mask.getchannel('A')\n",
        "    elif 'L' in pil_mask.getbands():\n",
        "        pil_mask = pil_mask.getchannel('L')\n",
        "    else:\n",
        "        raise RuntimeError('Mask must have an alpha channel or be one channel')\n",
        "    mask = TF.to_tensor(pil_mask.resize((toksX, toksY), Image.BILINEAR))\n",
        "    mask = mask.to(device).unsqueeze(0)\n",
        "    mask_dist = TF.to_tensor(pil_mask.resize((sideX, sideY), Image.BILINEAR)).to(device).unsqueeze(0)\n",
        "\n",
        "    #Threshold on the average of the mask\n",
        "    std, mean = torch.std_mean(mask_dist.view(-1)[torch.nonzero(mask_dist.view(-1))])\n",
        "    std = std.item()\n",
        "    mean = mean.item()\n",
        "    print(mean + (0.5) * std)\n",
        "    mask = mask.lt(mean).float()\n",
        "\n",
        "    if invert_mask:\n",
        "        mask = 1 - mask\n",
        "    mask_total = mask_dist.view(-1).sum()\n",
        "else:\n",
        "    mask = torch.ones([], device=device)\n",
        "\n",
        "z = model.quant_conv(model.encoder(image * 2 - 1))\n",
        "z.requires_grad_()\n",
        "opt = optim.Adam([z], lr=0.15)\n",
        "\n",
        "#Draw picture\n",
        "def synth(z, sample=False):\n",
        "    logits = model.quantize.proj(z)\n",
        "    if sample:\n",
        "        one_hot = F.gumbel_softmax(logits, tau=1, hard=True, dim=1)\n",
        "    else:\n",
        "        one_hot = F.one_hot(logits.argmax(1), logits.shape[1]).movedim(3, 1).to(logits.dtype)\n",
        "    z_q = torch.einsum('nchw,cd->ndhw', one_hot, model.quantize.embed.weight)\n",
        "    return clamp_with_grad(model.decode(z_q).add(1).div(2), 0, 1)\n",
        "\n",
        "#Draw picture + print status + save picture\n",
        "@torch.no_grad()\n",
        "def checkin(i, losses):\n",
        "    losses_str = ', '.join(f'{loss.item():g}' for loss in losses)\n",
        "    tqdm.write(f'i: {i}, loss: {sum(losses).item():g}, losses: {losses_str}')\n",
        "    out = synth(z)\n",
        "    TF.to_pil_image(out[0].cpu()).save('progress.png')\n",
        "    display.display(display.Image('progress.png'))\n",
        "\n",
        "\n",
        "#Optimize for prompt\n",
        "def ascend_txt():\n",
        "    out = synth(replace_grad(z, z * mask), sample=True)\n",
        "    seed = torch.randint(2**63 - 1, [])\n",
        "  \n",
        "    noise_val = (1 - t) * 0.1\n",
        "\n",
        "    #Random crops\n",
        "    with torch.random.fork_rng():\n",
        "        torch.manual_seed(seed)\n",
        "        out_embeds = perceptor.encode_image(normalize(make_cutouts(out))).float()\n",
        "\n",
        "    with torch.random.fork_rng():\n",
        "        torch.manual_seed(seed)\n",
        "        image_embeds = perceptor.encode_image(normalize(make_cutouts(image))).float()\n",
        "\n",
        "    if mask_dist is not None:\n",
        "        with torch.random.fork_rng():\n",
        "            torch.manual_seed(seed)\n",
        "            mask_scores = make_cutouts(mask_dist, augs=False, grads=False).view(cut_out_num, -1).sum(dim=-1) / mask_total\n",
        "\n",
        "    result = []\n",
        "    #Compare the image we started with to crops of the current image\n",
        "    image_analogy = spherical_dist(out_embeds, image_embeds) * (torch.ones_like(mask_scores) - mask_scores)\n",
        "    result.append(image_analogy.mean())\n",
        "    #Move over a spherical geodesic that connects the \"from state\" to the \"to state\"\n",
        "    word_analogy = (spherical_dist(out_embeds, to_embed, noise=False, noise_coeff=noise_val) - spherical_dist(out_embeds, from_embed, noise=False, noise_coeff=noise_val))\n",
        "    result.append(word_analogy.mean() * scale_dir_by)\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def train(i):\n",
        "    global t\n",
        "    t = min(float(i)/float(cut_pow_length),1.0) \n",
        "    cur_cut_pow = (1 - t) * cut_pow_start + t * cut_pow_end\n",
        "    make_cutouts.set_cut_pow(cur_cut_pow)\n",
        "\n",
        "    global scale_dir_by\n",
        "    #scale_dir_by = clamp(1.0, sdbo * (1 - t) + t, sdbo)\n",
        "\n",
        "    opt.zero_grad()\n",
        "    lossAll = ascend_txt()\n",
        "    if i % 50 == 0:\n",
        "        checkin(i, lossAll)\n",
        "    loss = sum(lossAll)\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "i = 0\n",
        "try:\n",
        "    with tqdm() as pbar:\n",
        "        while True:\n",
        "            train(i)\n",
        "            i += 1\n",
        "            pbar.update()\n",
        "except KeyboardInterrupt:\n",
        "    pass\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}